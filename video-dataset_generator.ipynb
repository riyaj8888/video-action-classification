{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694b87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec tf_gpu in /home/ubuntu/.local/share/jupyter/kernels/tf_gpu\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !python -m ipykernel install --user --name=tf_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593c407",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "### 1. split UCF101 dataset into train/test folder\n",
    "### 2. create activity_data folder with train/test subfolder\n",
    "### 3. extract frames from videos and save them into activity_data folder train/test\n",
    "### 4. create data_files folder to store csv files of each video's frame\n",
    "### 5. create train/test folder inside data_files folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61fd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2,glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43675046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec6ffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      video_name\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the .txt file which have names of training videos\n",
    "f = open(\"/home/ubuntu/ucf_model/ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "train_videos = temp.split('\\n')\n",
    "\n",
    "# creating a dataframe having video names\n",
    "train = pd.DataFrame()\n",
    "train['video_name'] = train_videos\n",
    "train = train[:-1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64b0dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mode = 'train/'\n",
    "\n",
    "# src_path = '/home/ubuntu/ucf_model/UCF-101/'\n",
    "# dest_path = '/home/ubuntu/ucf_model/'+mode\n",
    "\n",
    "# for vid in tqdm(train_videos):\n",
    "#     vid = vid.split(' ')[0]\n",
    "# #     print(vid)\n",
    "#     shutil.copy(src_path+vid ,dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a77285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    video_name\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the .txt file which have names of test videos\n",
    "f = open(\"/home/ubuntu/ucf_model/ucfTrainTestlist/testlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "test_videos = temp.split('\\n')\n",
    "\n",
    "# creating a dataframe having video names\n",
    "test = pd.DataFrame()\n",
    "test['video_name'] = test_videos\n",
    "test = test[:-1]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4745f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "\n",
    "src_path = '/home/ubuntu/ucf_model/UCF-101/'\n",
    "dest_path = '/home/ubuntu/ucf_model/UCF101/'+mode+'/'\n",
    "vid_data_frame = test_videos\n",
    "\n",
    "def copy_train_test_data(vid_data_frame,src_path,dest_path,mode):\n",
    "    \n",
    "    for vid in tqdm(vid_data_frame):\n",
    "        if mode == 'train':\n",
    "            vid = vid.split(' ')[0]\n",
    "        else:\n",
    "            vid=vid\n",
    "        \n",
    "        shutil.copy(src_path+vid ,dest_path)\n",
    "        \n",
    "# copy_train_test_data(vid_data_frame,src_path,dest_path,mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d2b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of videos in train = 9537\n"
     ]
    }
   ],
   "source": [
    "mode = 'train'\n",
    "dest_path = '/home/ubuntu/ucf_model/UCF101/'+mode+'/'\n",
    "\n",
    "videos_len = glob.glob(dest_path+'*')\n",
    "print(\"num of videos in {} = {}\".format(mode,len(videos_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20bbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_frames(video,output_path):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video) # initialize a cap object for reading the video\n",
    "\n",
    "    ret=True\n",
    "    frame_num=0\n",
    "    while ret:\n",
    "        ret, img = cap.read()\n",
    "        output_file_name = 'img_{:06d}'.format(frame_num) + '.png' # img_000001.png\n",
    "        # output frame to write 'activity_data/train/Archery/v_Archery_g01_c01/img_000001.png'\n",
    "        output_file_path = os.path.join(output_path, output_file_name)\n",
    "        frame_num += 1\n",
    "        print(\"Frame no. \",frame_num)\n",
    "        try:\n",
    "    #                 cv2.imshow('img',img)\n",
    "    #                 cv2.waitKey(5)\n",
    "            cv2.imwrite(output_file_path, img) # writing frames to defined location\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        if ret==False:                \n",
    "            cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550e946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9537\n"
     ]
    }
   ],
   "source": [
    "root_dir_train = '/home/ubuntu/ucf_model/UCF101/train/'\n",
    "data_dir_list_train = glob.glob(root_dir_train+'*')\n",
    "print(len(data_dir_list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebd8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3783\n"
     ]
    }
   ],
   "source": [
    "root_dir_test = '/home/ubuntu/ucf_model/UCF101/test/'\n",
    "data_dir_list_test = glob.glob(root_dir_test+'*')\n",
    "print(len(data_dir_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7b9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_list = ['Archery', 'Basketball', 'Biking']\n",
    "\n",
    "path = '/home/ubuntu/ucf_model/activity_data/train/'\n",
    "root_dir = root_dir_train\n",
    "dest_dir = '/home/ubuntu/ucf_model/activity_data/'\n",
    "\n",
    "def vid_to_frames(activity_list,path,root_dir,dest_dir):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    # To list what are the directories - train, test\n",
    "    data_dir_list = glob.glob(root_dir+'*')\n",
    "    \n",
    "    i=0\n",
    "    for video in data_dir_list: # read the train and test directory one by one\n",
    "        \n",
    "        activity_label = video.split('/')[-1]\n",
    "        activity_label = activity_label.split('.')[0]\n",
    "        activity_label = activity_label.split('_')[1]\n",
    "        video_ = video.split('/')[-1]\n",
    "        video_ = video_.split('.')[0]\n",
    "        \n",
    "        if activity_label in activity_list:\n",
    "            \n",
    "            print(activity_label)\n",
    "                        \n",
    "            if not os.path.exists(path+activity_label):\n",
    "                os.mkdir(path+activity_label)\n",
    "            \n",
    "            if not os.path.exists(path+activity_label+'/'+video_):\n",
    "\n",
    "                os.mkdir(path+activity_label+'/'+video_)\n",
    "            output_path = path+activity_label+'/'+video_+'/'\n",
    "            write_frames(video,output_path)\n",
    "\n",
    "# vid_to_frames(activity_list,path,root_dir,dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9bbd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_list = ['Archery', 'Basketball', 'Biking']\n",
    "\n",
    "path = '/home/ubuntu/ucf_model/activity_data/test/'\n",
    "root_dir = root_dir_test #ucf test data directory\n",
    "dest_dir = '/home/ubuntu/ucf_model/activity_data/'\n",
    "\n",
    "def vid_to_frames(activity_list,path,root_dir,dest_dir):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    # To list what are the directories - train, test\n",
    "    data_dir_list = glob.glob(root_dir+'*')\n",
    "    \n",
    "    i=0\n",
    "    for video in data_dir_list: # read the train and test directory one by one\n",
    "        \n",
    "        activity_label = video.split('/')[-1]\n",
    "        activity_label = activity_label.split('.')[0]\n",
    "        activity_label = activity_label.split('_')[1]\n",
    "        video_ = video.split('/')[-1]\n",
    "        video_ = video_.split('.')[0]\n",
    "        \n",
    "        if activity_label in activity_list:\n",
    "            \n",
    "            print(activity_label)\n",
    "                        \n",
    "            if not os.path.exists(path+activity_label):\n",
    "                os.mkdir(path+activity_label)\n",
    "            \n",
    "            if not os.path.exists(path+activity_label+'/'+video_):\n",
    "\n",
    "                os.mkdir(path+activity_label+'/'+video_)\n",
    "            output_path = path+activity_label+'/'+video_+'/'\n",
    "            write_frames(video,output_path)\n",
    "\n",
    "# vid_to_frames(activity_list,path,root_dir,dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7b8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_list = ['Archery', 'Basketball', 'Biking']\n",
    "\n",
    "# path = '/home/ubuntu/ucf_model/activity_data/test/'\n",
    "# root_dir = root_dir_test\n",
    "# dest_dir = '/home/ubuntu/ucf_model/activity_data/'\n",
    "\n",
    "# vid_to_frames(activity_list,path,root_dir,dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd0fcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigning labels to each flower category\n",
    "num_classes = 3 \n",
    "labels_name={'Archery':0,'Basketball':1,'Biking':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a08d9ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ucf_model/activity_data/train/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop over every activity category in train and test \n",
    "train_data_path = '/home/ubuntu/ucf_model/activity_data/train/'#activity train data directory\n",
    "test_data_path = '/home/ubuntu/ucf_model/activity_data/test/' #activity test data directory\n",
    "\n",
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "617bf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = '/home/ubuntu/ucf_model/'\n",
    "\n",
    "# if not os.path.exists(root_data_dir+'data_files'):\n",
    "#     os.mkdir(root_data_dir+'data_files')\n",
    "# if not os.path.exists('data_files/train'):\n",
    "#     os.mkdir(root_data_dir+'data_files/train') \n",
    "# if not os.path.exists(root_data_dir+'data_files/test'):\n",
    "#     os.mkdir(root_data_dir+'data_files/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dd54a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basketball', 'Biking', 'Archery']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir_list = os.listdir(test_data_path)\n",
    "data_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecb01e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_list = os.listdir(test_data_path)\n",
    "# for data_dir in data_dir_list: # looping over every activity\n",
    "#     label = labels_name[str(data_dir)]\n",
    "#     video_list = os.listdir(os.path.join(test_data_path,data_dir))\n",
    "#     for vid in video_list: # looping over every video within an activity\n",
    "#         train_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n",
    "#         img_list = os.listdir(os.path.join(test_data_path,data_dir,vid))\n",
    "#         for img in img_list:# looping over every frame within the video\n",
    "#             img_path = os.path.join(test_data_path,data_dir,vid,img)\n",
    "#             train_df = train_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n",
    "#         file_name='{}_{}.csv'.format(data_dir,vid)\n",
    "#         train_df.to_csv(root_data_dir+'data_files/test/{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b6fe661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "f = os.listdir(root_data_dir+'data_files/test/')\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e6a6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_list = os.listdir(train_data_path)\n",
    "# for data_dir in data_dir_list: # looping over every activity\n",
    "#     label = labels_name[str(data_dir)]\n",
    "#     video_list = os.listdir(os.path.join(train_data_path,data_dir))\n",
    "#     for vid in video_list: # looping over every video within an activity\n",
    "#         test_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n",
    "#         img_list = os.listdir(os.path.join(train_data_path,data_dir,vid))\n",
    "#         for img in img_list:# looping over every frame within the video\n",
    "#             img_path = os.path.join(train_data_path,data_dir,vid,img)\n",
    "#             test_df = test_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n",
    "#         file_name='{}_{}.csv'.format(data_dir,vid)\n",
    "#         test_df.to_csv(root_data_dir+'data_files/train/{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc9d53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "f = os.listdir(root_data_dir+'data_files/train/')\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "833b6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from collections import deque\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54bc13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the video files from the csv file\n",
    "def file_generator(data_path,data_files,temporal_stride=1,temporal_length=16):\n",
    "    '''\n",
    "    data_files - list of csv files to be read.\n",
    "    '''\n",
    "    for f in data_files: # read all the csv files (one csv file corresponds to one vdieo) in data_files one by one\n",
    "        tmp_df = pd.read_csv(os.path.join(data_path,f))\n",
    "        label_list = list(tmp_df['Label'])  # Load all the labels in the label_list\n",
    "#         print(label_list,label_list[0])\n",
    "        total_images = len(label_list) \n",
    "        if total_images>=temporal_length: # only if the number of frames in the video is greater tha temporal length, use that video\n",
    "            num_samples = int((total_images-temporal_length)/temporal_stride)+1\n",
    "#             print ('num of samples from vid seq-{}: {}'.format(f,num_samples))\n",
    "            img_list = list(tmp_df['FileName'])\n",
    "        else: # if the number of frames are less than temporal length , discard it\n",
    "#             print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))\n",
    "            continue\n",
    "\n",
    "        start_frame = 0\n",
    "        samples = deque() # initliaze a queue to store the frames \n",
    "        samp_count=0 # a counter to count the number of smaple. one smaple has as many frames as defined by temporal length\n",
    "        for img in img_list:\n",
    "            samples.append(img)\n",
    "            if len(samples)==temporal_length: #if the queue has as many frames as temporal length, return it as one sample\n",
    "                samples_c=copy.deepcopy(samples) # copy the queue as in the next stage frames would be popped\n",
    "                samp_count+=1\n",
    "                for t in range(temporal_stride): # pop out as many frames as described by the stride from the left to accomodate new frames\n",
    "                    samples.popleft()\n",
    "                yield samples_c,label_list[0] # return a sample(consisting of as many frames as defined by temporal length) \n",
    "                                                # and its corsponding label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "899e3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the samples and their corresponding label for each video\n",
    "# base_path =  provide path to csv data files\n",
    "# data_files =  listdir of csv files\n",
    "\n",
    "def load_samples(base_path = root_data_dir+'data_files/',data_cat='test',temporal_stride=1,temporal_length=16):\n",
    "    \n",
    "    data_path = os.path.join(base_path,data_cat)\n",
    "    data_files = os.listdir(data_path)\n",
    "    # define a generator to read the samples\n",
    "    file_gen = file_generator(data_path,data_files,temporal_stride,temporal_length)\n",
    "    iterator = True\n",
    "    data_list = []\n",
    "    while iterator:\n",
    "        try:\n",
    "            x,y = next(file_gen)\n",
    "            x=list(x)\n",
    "            data_list.append([x,y])\n",
    "        except Exception as e:\n",
    "            print ('the exception: ',e)\n",
    "            iterator = False\n",
    "            print ('end of data generator')\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eecd05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the exception:  \n",
      "end of data generator\n"
     ]
    }
   ],
   "source": [
    "train_data = load_samples(data_cat='train',temporal_stride=4,temporal_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "384095ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train samples: 12362\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of train samples:',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ee7466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the exception:  \n",
      "end of data generator\n"
     ]
    }
   ],
   "source": [
    "test_data = load_samples(data_cat='test',temporal_stride=4,temporal_length=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b9fd1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train samples: 4387\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of train samples:',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e01592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(samples):\n",
    "    data = shuffle(samples,random_state=2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9f020b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(img):\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae4c5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data,batch_size=10,temporal_padding='same',shuffle=True):              \n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    data is an array [[img1_filename,img2_filename...,img16_filename],label1], [image2_filename,label2],...].\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    if shuffle:\n",
    "        data = shuffle_data(data)\n",
    "    while True:   \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            print ('startring index: ', offset) \n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = data[offset:offset+batch_size]\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            # For each example\n",
    "            for batch_sample in batch_samples: # Loop over every batch\n",
    "                # Load image (X)\n",
    "                x = batch_sample[0]\n",
    "                y = batch_sample[1]\n",
    "                temp_data_list = []\n",
    "                for img in x:\n",
    "                    try:\n",
    "                        img = cv2.imread(img)\n",
    "                        #apply any kind of preprocessing here\n",
    "                        #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "                        img = preprocess_image(img)\n",
    "                        temp_data_list.append(img)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print (e)\n",
    "                        print ('error reading file: ',img)                      \n",
    "                # Read label (y)\n",
    "                #label = label_names[y]\n",
    "                # Add example to arrays\n",
    "                X_train.append(temp_data_list)\n",
    "                y_train.append(y)\n",
    "    \n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            #X_train = np.rollaxis(X_train,1,4)\n",
    "            y_train = np.array(y_train)\n",
    "            # convert to one hot encoding for training keras model\n",
    "            y_train = np.eye(3)[y_train]\n",
    "#             y_train = np_utils.to_categorical(y_train, 3)\n",
    "    \n",
    "            # yield the next training batch            \n",
    "            yield X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1ad0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(train_data,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "513e0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startring index:  0\n",
      "x shape:  (4, 16, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "x,y = next(train_generator)\n",
    "print ('x shape: ',x.shape)\n",
    "# print ('y shape: ',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "154d6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_0=x[2]\n",
    "# y_0=y[2]\n",
    "# print('x_0 shape: ',x_0.shape)\n",
    "# print('y_0 shape: ',y_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e1895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=3\n",
    "labels_to_class = {0:'Archery',1:'Basketball',2:'Biking'}\n",
    "class_to_labels = {'Archery':0,'Basketball':1,'Biking':2}\n",
    "resize = 224\n",
    "num_epochs =10\n",
    "batch_size =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb253924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity = labels_to_class[np.argmax(y_0)]\n",
    "# activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22ea925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_images=16\n",
    "# fig=plt.figure(figsize=(8,8))\t\n",
    "# plt.title(\"one sample with {} frames ; activity:{}\".format(num_of_images,activity))\n",
    "# subplot_num = int(np.ceil(np.sqrt(num_of_images)))\n",
    "# for i in range(int(num_of_images)):\n",
    "#     ax = fig.add_subplot(subplot_num, subplot_num, i+1)\n",
    "#     #ax.imshow(output_image[0,:,:,i],interpolation='nearest' ) #to see the first filter\n",
    "#     ax.imshow(x_0[i,:,:,::-1])\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae1b8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from collections import deque\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "# from config import Config\n",
    "\n",
    "class ActionDataGenerator(object):\n",
    "    \n",
    "    def __init__(self,root_data_path,temporal_stride=1,temporal_length=16,resize=256):\n",
    "        \n",
    "        self.root_data_path = root_data_path\n",
    "        self.temporal_length = temporal_length\n",
    "        self.temporal_stride = temporal_stride\n",
    "        self.resize=resize\n",
    "    def file_generator(self,data_path,data_files):\n",
    "        '''\n",
    "        data_files - list of csv files to be read.\n",
    "        '''\n",
    "        for f in data_files:       \n",
    "            tmp_df = pd.read_csv(os.path.join(data_path,f))\n",
    "            label_list = list(tmp_df['Label'])\n",
    "            total_images = len(label_list) \n",
    "            if total_images>=self.temporal_length:\n",
    "                num_samples = int((total_images-self.temporal_length)/self.temporal_stride)+1\n",
    "#                 print ('num of samples from vid seq-{}: {}'.format(f,num_samples))\n",
    "                img_list = list(tmp_df['FileName'])\n",
    "            else:\n",
    "#                 print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))\n",
    "                continue\n",
    "            \n",
    "            start_frame = 0\n",
    "            samples = deque()\n",
    "            samp_count=0\n",
    "            for img in img_list:\n",
    "                samples.append(img)\n",
    "                if len(samples)==self.temporal_length:\n",
    "                    samples_c=copy.deepcopy(samples)\n",
    "                    samp_count+=1\n",
    "                    for t in range(self.temporal_stride):\n",
    "                        samples.popleft() \n",
    "                    yield samples_c,label_list[0]\n",
    "\n",
    "    def load_samples(self,data_cat='train'):\n",
    "        data_path = os.path.join(self.root_data_path,data_cat)\n",
    "        csv_data_files = os.listdir(data_path)\n",
    "        file_gen = self.file_generator(data_path,csv_data_files)\n",
    "        iterator = True\n",
    "        data_list = []\n",
    "        while iterator:\n",
    "            try:\n",
    "                x,y = next(file_gen)\n",
    "                x=list(x)\n",
    "                data_list.append([x,y])\n",
    "            except Exception as e:\n",
    "#                 print ('the exception: ',e)\n",
    "                iterator = False\n",
    "#                 print ('end of data generator')\n",
    "        return data_list\n",
    "    \n",
    "    def shuffle_data(self,samples):\n",
    "        data = shuffle(samples,random_state=2)\n",
    "        return data\n",
    "    \n",
    "    def preprocess_image(self,img):\n",
    "        img = cv2.resize(img,(self.resize,self.resize))\n",
    "        img = img/255\n",
    "        return img\n",
    "    \n",
    "    def data_generator(self,data,batch_size=10,shuffle=True):              \n",
    "        \"\"\"\n",
    "        Yields the next training batch.\n",
    "        data is an array [[img1_filename,img2_filename...,img16_filename],label1], [image2_filename,label2],...].\n",
    "        \"\"\"\n",
    "        num_samples = len(data)\n",
    "        if shuffle:\n",
    "            data = self.shuffle_data(data)\n",
    "        while True:   \n",
    "            for offset in range(0, num_samples, batch_size):\n",
    "                #print ('startring index: ', offset) \n",
    "                # Get the samples you'll use in this batch\n",
    "                batch_samples = data[offset:offset+batch_size]\n",
    "                # Initialise X_train and y_train arrays for this batch\n",
    "                X_train = []\n",
    "                y_train = []\n",
    "                # For each example\n",
    "                for batch_sample in batch_samples:\n",
    "                    # Load image (X)\n",
    "                    x = batch_sample[0]\n",
    "                    y = batch_sample[1]\n",
    "                    temp_data_list = []\n",
    "                    for img in x:\n",
    "                        \n",
    "                        try:\n",
    "                            img = cv2.imread(img)\n",
    "                            #apply any kind of preprocessing here\n",
    "                            #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "                            img = self.preprocess_image(img)\n",
    "                            temp_data_list.append(img)\n",
    "    \n",
    "                        except Exception as e:\n",
    "                            print (e)\n",
    "#                             print ('error reading file: ',img)  \n",
    "    \n",
    "                    # Read label (y)\n",
    "                    #label = label_names[y]\n",
    "                    # Add example to arrays\n",
    "                    X_train.append(temp_data_list)\n",
    "                    y_train.append(y)\n",
    "        \n",
    "                # Make sure they're numpy arrays (as opposed to lists)\n",
    "                X_train = np.array(X_train)\n",
    "                #X_train = np.rollaxis(X_train,1,4)\n",
    "                y_train = np.array(y_train)\n",
    "                y_train = np.eye(3)[y_train]\n",
    "\n",
    "                # The generator-y part: yield the next training batch            \n",
    "                yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "852cbd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train_samples: 12362\n"
     ]
    }
   ],
   "source": [
    "root_data_path = '/home/ubuntu/ucf_model/data_files/'\n",
    "data_gen_obj=ActionDataGenerator(root_data_path,temporal_stride=4,temporal_length=16)\n",
    "train_data = data_gen_obj.load_samples(data_cat='train')\n",
    "\n",
    "print('num of train_samples: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69b67765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of test_samples: 4387\n"
     ]
    }
   ],
   "source": [
    "root_data_path = '/home/ubuntu/ucf_model/data_files/'\n",
    "data_gen_obj=ActionDataGenerator(root_data_path,temporal_stride=4,temporal_length=16)\n",
    "train_data = data_gen_obj.load_samples(data_cat='test')\n",
    "\n",
    "print('num of test_samples: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f555075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Activation, Conv3D, Input,Dense, Dropout, Flatten,\n",
    "                          MaxPooling3D)\n",
    "# from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe085023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "def actmodel(num_classes=3):\n",
    "    # Define model\n",
    "    # model = Sequential()\n",
    "    x_i = Input(shape=(16,224,224,3))\n",
    "    x = Conv3D(32, kernel_size=(3, 3, 3),  padding='same')(x_i)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv3D(32, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling3D(pool_size=(3, 3, 3), padding='same')(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "\n",
    "    x=Conv3D(64, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv3D(64, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling3D(pool_size=(3, 3, 3), padding='same')(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(512, activation='relu')(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    x=Dense(num_classes)(x)\n",
    "    y=Activation(\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs = x_i,outputs=y)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97119bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = actmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78443bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "403286fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow  as tf\n",
    "\n",
    "def TD_ConvBNRelu(x,filters=32,kernel_size=3,stride=2):\n",
    "\n",
    "  L1 = Conv2D(filters=filters,kernel_size=kernel_size,strides=stride)\n",
    "  L1_x = TimeDistributed(L1)(x)\n",
    "\n",
    "  L2 = BatchNormalization()\n",
    "  L2_x = TimeDistributed(L2)(L1_x)\n",
    "\n",
    "  L3 = ReLU()\n",
    "  L3_x = TimeDistributed(L3)(L2_x)\n",
    "  \n",
    "  return L3_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9a8852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_CNN_MSA(num_of_frames = 8):\n",
    "\n",
    "  x_ip = Input(shape=(num_of_frames,256,256,3))\n",
    "\n",
    "  x = TD_ConvBNRelu(x_ip,32,3,1)\n",
    "  x = TD_ConvBNRelu(x,32,3,2)\n",
    "\n",
    "  x = TD_ConvBNRelu(x,64,3,1)\n",
    "  x = TD_ConvBNRelu(x,64,3,2)\n",
    "\n",
    "  x = TD_ConvBNRelu(x,128,3,1)\n",
    "  x = TD_ConvBNRelu(x,128,3,2)\n",
    "\n",
    "  x = TD_ConvBNRelu(x,256,3,1)\n",
    "  x = TD_ConvBNRelu(x,256,3,2)\n",
    "\n",
    "  x = TD_ConvBNRelu(x,512,3,1)\n",
    "  x = TD_ConvBNRelu(x,512,3,2)\n",
    "\n",
    "  x = TD_ConvBNRelu(x,1024,3,1)\n",
    "  x = TD_ConvBNRelu(x,1024,3,2)\n",
    "\n",
    "\n",
    "  x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "  MSA = tf.keras.layers.MultiHeadAttention(num_heads=12, key_dim=128)\n",
    "\n",
    "  x = MSA(x,x)\n",
    "\n",
    "  embedding_sum = Lambda(lambda x: K.sum(x, axis=1))(x)\n",
    "\n",
    "  y = Dense(3,activation='softmax')(embedding_sum)\n",
    "\n",
    "  model = tf.keras.models.Model(inputs=x_ip,outputs = y )\n",
    "\n",
    "  model.summary()\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6050645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 16, 256, 256 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_74 (TimeDistri (None, 16, 254, 254, 896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_75 (TimeDistri (None, 16, 254, 254, 128         time_distributed_74[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_76 (TimeDistri (None, 16, 254, 254, 0           time_distributed_75[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_77 (TimeDistri (None, 16, 126, 126, 9248        time_distributed_76[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_78 (TimeDistri (None, 16, 126, 126, 128         time_distributed_77[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_79 (TimeDistri (None, 16, 126, 126, 0           time_distributed_78[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_80 (TimeDistri (None, 16, 124, 124, 18496       time_distributed_79[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_81 (TimeDistri (None, 16, 124, 124, 256         time_distributed_80[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_82 (TimeDistri (None, 16, 124, 124, 0           time_distributed_81[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_83 (TimeDistri (None, 16, 61, 61, 6 36928       time_distributed_82[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_84 (TimeDistri (None, 16, 61, 61, 6 256         time_distributed_83[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_85 (TimeDistri (None, 16, 61, 61, 6 0           time_distributed_84[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_86 (TimeDistri (None, 16, 59, 59, 1 73856       time_distributed_85[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_87 (TimeDistri (None, 16, 59, 59, 1 512         time_distributed_86[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_88 (TimeDistri (None, 16, 59, 59, 1 0           time_distributed_87[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_89 (TimeDistri (None, 16, 29, 29, 1 147584      time_distributed_88[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_90 (TimeDistri (None, 16, 29, 29, 1 512         time_distributed_89[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_91 (TimeDistri (None, 16, 29, 29, 1 0           time_distributed_90[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_92 (TimeDistri (None, 16, 27, 27, 2 295168      time_distributed_91[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_93 (TimeDistri (None, 16, 27, 27, 2 1024        time_distributed_92[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_94 (TimeDistri (None, 16, 27, 27, 2 0           time_distributed_93[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_95 (TimeDistri (None, 16, 13, 13, 2 590080      time_distributed_94[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_96 (TimeDistri (None, 16, 13, 13, 2 1024        time_distributed_95[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_97 (TimeDistri (None, 16, 13, 13, 2 0           time_distributed_96[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_98 (TimeDistri (None, 16, 11, 11, 5 1180160     time_distributed_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_99 (TimeDistri (None, 16, 11, 11, 5 2048        time_distributed_98[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_100 (TimeDistr (None, 16, 11, 11, 5 0           time_distributed_99[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_101 (TimeDistr (None, 16, 5, 5, 512 2359808     time_distributed_100[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_102 (TimeDistr (None, 16, 5, 5, 512 2048        time_distributed_101[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_103 (TimeDistr (None, 16, 5, 5, 512 0           time_distributed_102[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_104 (TimeDistr (None, 16, 3, 3, 102 4719616     time_distributed_103[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_105 (TimeDistr (None, 16, 3, 3, 102 4096        time_distributed_104[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_106 (TimeDistr (None, 16, 3, 3, 102 0           time_distributed_105[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_107 (TimeDistr (None, 16, 1, 1, 102 9438208     time_distributed_106[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_108 (TimeDistr (None, 16, 1, 1, 102 4096        time_distributed_107[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_109 (TimeDistr (None, 16, 1, 1, 102 0           time_distributed_108[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_110 (TimeDistr (None, 16, 1024)     0           time_distributed_109[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 16, 1024)     6297088     time_distributed_110[0][0]       \n",
      "                                                                 time_distributed_110[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1024)         0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            3075        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 25,186,339\n",
      "Trainable params: 25,178,275\n",
      "Non-trainable params: 8,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TD_CNN_MSA(num_of_frames = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8a44b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = '/home/ubuntu/ucf_model/data_files/'\n",
    "\n",
    "data_gen_obj=ActionDataGenerator(root_data_path,temporal_stride=4,temporal_length=16)\n",
    "\n",
    "\n",
    "train_data = data_gen_obj.load_samples(data_cat='train')\n",
    "\n",
    "test_data = data_gen_obj.load_samples(data_cat='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e585f21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train_samples: 12362\n",
      "num of test_samples: 4387\n"
     ]
    }
   ],
   "source": [
    "print('num of train_samples: {}'.format(len(train_data)))\n",
    "print('num of test_samples: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec65a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_gen_obj.data_generator(train_data,batch_size=6,shuffle=True)\n",
    "\n",
    "test_generator = data_gen_obj.data_generator(test_data,batch_size=6,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "500a8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8874fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4/12362 [..............................] - ETA: 36:14:56 - loss: 123.2302 - accuracy: 0.4167"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1525f0e84995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit model using generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m hist = model.fit(train_generator, \n\u001b[0;32m----> 3\u001b[0;31m                 steps_per_epoch=len(train_data),validation_data = test_generator,validation_steps=len(test_data),epochs=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/test_Env_gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit model using generator\n",
    "hist = model.fit(train_generator, \n",
    "                steps_per_epoch=len(train_data),validation_data = test_generator,validation_steps=len(test_data),epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05b5a7e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moviepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e90b3640d5d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'moviepy'"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd35a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
